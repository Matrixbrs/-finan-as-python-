# -*- coding: utf-8 -*-
"""Cópia de Untitled48.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8e4rc0tasNpJ9YgCP6_ike6i8M_5vFm
"""

from typing_extensions import OrderedDict
!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install seaborn
!pip install scikit-learn
!pip install yfinance
!pip install python-decouple
!pip install alpaca-py
!pip install backtesting
!pip install arch

# Import warnings
import warnings
warnings.filterwarnings('ignore')

# Import data manipulation libraries
import pandas as pd
import numpy as np
from pprint import pprint
from collections import OrderedDict
from numpy.linalg import multi_dot
from scipy import stats
from tabulate import tabulate

# Import Yahoo Finance
import yfinance as yf

# Import plotly express
import plotly.express as px
px.defaults.width, px.defaults.height = 1000, 600

# Set precision
pd.set_option('display.precision', 4)

# Specify assets / stocks with NSE suffix
assets = ['ICICIBANK.NS', 'ITC.NS', 'RELIANCE.NS', 'TCS.NS', 'ASIANPAINT.NS']
asset_names = ['ICICIBANK', 'ITC', 'RELIANCE', 'TCS', 'ASIANPAINT']
print("Assets to download:", assets)

# Download stock data from Yahoo Finance
# Set the period for data download (e.g., 2 years of data)
df = yf.download(assets, period="2y", interval="1d")['Close']

# Rename columns to remove .NS suffix for cleaner column names
df.columns = asset_names

# View dataframe
print(f"Data shape: {df.shape}")
print(f"Date range: {df.index[0]} to {df.index[-1]}")
df.head()

# Handle any missing data
df = df.dropna()
print(f"Data shape after removing NaN: {df.shape}")
df.tail()

# Calculate daily returns
returns = df.pct_change().dropna()
print(f"Returns shape: {returns.shape}")
returns.head()

#parametrização Var

# Stock returns for ICICIBANK
stockreturn = returns['ICICIBANK']

# Calculate mean and standard deviation
mean = np.mean(stockreturn)
stdev = np.std(stockreturn)

print(f"Mean daily return: {mean:.6f}")
print(f"Daily volatility: {stdev:.6f}")

# Calculate VaR at different confidence levels
VaR_90 = stats.norm.ppf(1-0.90, mean, stdev)
VaR_95 = stats.norm.ppf(1-0.95, mean, stdev)
VaR_99 = stats.norm.ppf(1-0.99, mean, stdev)

# Output results in tabular format
table = [['90%', VaR_90], ['95%', VaR_95], ['99%', VaR_99]]
header = ['Confidence Level', 'Value At Risk']
print(tabulate(table, headers=header, floatfmt='.6f'))

#Historico Var

# Use quantile function for Historical VaR
hVaR_90 = returns['ICICIBANK'].quantile(0.10)
hVaR_95 = returns['ICICIBANK'].quantile(0.05)
hVaR_99 = returns['ICICIBANK'].quantile(0.01)

# Output results in tabular format
htable = [['90%', hVaR_90], ['95%', hVaR_95], ['99%', hVaR_99]]
print(tabulate(htable, headers=header, floatfmt='.6f'))

#Monte carlos Var

# Set seed for reproducibility
np.random.seed(42)

# Number of simulations
n_sims = 5000

# Simulate returns and sort
sim_returns = np.random.normal(mean, stdev, n_sims)

# Use percentile function for MCVaR
MCVaR_90 = np.percentile(sim_returns, 10)
MCVaR_95 = np.percentile(sim_returns, 5)
MCVaR_99 = np.percentile(sim_returns, 1)
# Output results in tabular format
mctable = [['90%', MCVaR_90], ['95%', MCVaR_95], ['99%', MCVaR_99]]
print(tabulate(mctable, headers=header, floatfmt='.6f'))

#Teste de Norma
# Normality test
shapiro_stat, shapiro_p = stats.shapiro(stockreturn)
print(f"Shapiro-Wilk Test:")
print(f"Statistic: {shapiro_stat:.6f}")
print(f"P-value: {shapiro_p:.6f}")
print(f"Result: {'Normal distribution' if shapiro_p > 0.05 else 'Not normal distribution'}")

# Anderson-Darling normality test
anderson_result = stats.anderson(stockreturn)
print(f"Anderson-Darling Test:")
print(f"Statistic: {anderson_result.statistic:.6f}")
print(f"Critical Values: {anderson_result.critical_values}")
print(f"Significance Levels: {anderson_result.significance_level}")

# Plot histogram
fig = px.histogram(returns, x='ICICIBANK', nbins=50,
                   histnorm='probability density',
                   title='Histogram of ICICIBANK Returns',
                   labels={'ICICIBANK': 'Daily Returns', 'count': 'Density'})
fig.show()

#Modificação Var

# First four moments
dist = OrderedDict({
    'Mean': np.mean(returns['ICICIBANK']),
    'Std Dev': np.std(returns['ICICIBANK']),
    'Skew': stats.skew(returns['ICICIBANK']),
    'Kurtosis': stats.kurtosis(returns['ICICIBANK'])
})
pprint(dist)

# Specify params for modified VaR
z = abs(stats.norm.ppf(0.01))
s = stats.skew(stockreturn)
k = stats.kurtosis(stockreturn)
t = z + 1/6*(z**2-1)*s + 1/24*(z**3-3*z)*k - 1/36*(2*z**3-5*z)*s**2

# Calculate VaR at difference confidence level
mVaR_99 = (mean - t*stdev)
print(f"Modified VaR (99%): {mVaR_99:.6f}")

# Specify params for modified VaR
z = abs(stats.norm.ppf(0.01))
s = stats.skew(stockreturn)
k = stats.kurtosis(stockreturn)
z = abs(stats.norm.ppf(0.01))
s = stats.skew(stockreturn)
k = stats.kurtosis(stockreturn)
t = z + 1/6*(z**2-1)*s + 1/24*(z**3-3*z)*k - 1/36*(2*z**3-5*z)*s**2

# Calculate VaR at difference confidence level
mVaR_99 = (mean - t*stdev)
print(f"Modified VaR (99%): {mVaR_99:.6f}")

# Plot Scaled VaR

horizons = range(1, 101)
sVaR = [-100*VaR_99*np.sqrt(x) for x in horizons]
sVaR_df = pd.DataFrame({'Horizon': horizons, 'ScaledVaR': sVaR})
fig = px.line(sVaR_df, x='Horizon', y='ScaledVaR',
              title='Scaled VaR over Different Time Horizons',
              labels={'EscalaVaR': 'Escala VaR (%)'})
fig.show()

#Condicional Value at risk(CVar)

# Calculate CVaR
CVaR_90 = returns['ICICIBANK'][returns['ICICIBANK'] <= hVaR_90].mean()
CVaR_95 = returns['ICICIBANK'][returns['ICICIBANK'] <= hVaR_95].mean()
CVaR_99 = returns['ICICIBANK'][returns['ICICIBANK'] <= hVaR_99].mean()
# Output results in tabular format
ctable = [['90%', CVaR_90], ['95%', CVaR_95], ['99%', CVaR_99]]
cheader = ['Confidence Level', 'Conditional Value At Risk']
print(tabulate(ctable, headers=cheader, floatfmt='.6f'))

#Portifilio Var

# Equal weights portfolio (you can modify these weights)
n_assets = len(asset_names)
wts = np.array([1/n_assets] * n_assets)  # Equal weights
print(f"Portfolio weights: {dict(zip(asset_names, wts))}")

# Portfolio mean returns and volatility
port_mean = wts.T @ returns.mean()
port_stdev = np.sqrt(multi_dot([wts.T, returns.cov(), wts]))
pVaR = stats.norm.ppf(1-0.99, port_mean, port_stdev)

print(f"Portfolio Mean: {port_mean:.6f}")
print(f"Portfolio Std Dev: {port_stdev:.6f}")
print(f"Portfolio VaR (99%): {pVaR:.6f}")

# Import arch library
from arch import arch_model
# Convert returns to percentage for better convergence
stockreturn_pct = stockreturn * 100

# Mean zero GARCH model
print("Fitting GARCH(1,1) model...")
g1 = arch_model(stockreturn_pct, vol='GARCH', mean='Zero', p=1, q=1, dist='Normal')
model = g1.fit(disp='off')  # disp='off' to suppress optimization output
print("Model fitted successfully!")

# Model output
print(model)

# Model summary
model.summary()

# Model parameters
print("Model Parameters:")
for param, value in model.params.items():
    print(f"{param}: {value:.6f}")

# Model Confidence Interval
conf_int = model.conf_int(alpha=.01)
print("99% Confidence Intervals:")
print(conf_int)

# Plot annualised volatility
fig = model.plot(annualize='D')
fig.show()

# Forecast for next 60 days
print("Generating volatility forecast...")

# Generate forecast
model_forecast = model.forecast(horizon=60)

# Extract variance forecast properly
forecast_var = model_forecast.variance.dropna()
forecast_values = forecast_var.iloc[-1, :].values  # Last row contains 60-day forecast

# Convert to annualized volatility percentage
forecast_vol_pct = np.sqrt(forecast_values * 252) * 100

# Create proper DataFrame
forecast_df = pd.DataFrame({
    'Days_Ahead': np.arange(1, len(forecast_vol_pct) + 1),
    'Volatility_Pct': forecast_vol_pct
})

# Create plot with explicit x and y
fig = px.line(forecast_df, x='Days_Ahead', y='Volatility_Pct',
              title='GARCH Volatility Forecast (60 Days)',
              labels={'Days_Ahead': 'Days Ahead',
                     'Volatility_Pct': 'Annualized Volatility (%)'})
fig.update_traces(line_color='red')
fig.show()